# `LisaModel` 代码阅读注释

## 模块定位
- `LisaModel` 继承 `mmengine.model.BaseModel`，是 LISA（Language-Image Segmentation Assistant）在 Sa2VA 项目的顶层封装，实现多模态大模型 (`mllm`) ?Grounding-SAM 分割头的协同训练与推理?- 类内部组合了?  - 一个可配置的大语言视觉模型 `self.mllm`（来?xtuner 的注册表）?  - 文本分词?`self.tokenizer`，用于追加分割特?token?  - `self.grounding_encoder`，通常?Grounding-SAM，负责把图像特征转换为可用于掩码解码的表示?  - 额外的前馈层 `self.text_hidden_fcs` 将语言隐藏态转换为 SAM mask decoder 所需的维度?  - 两个损失函数组件（比?BCE + Dice），用于训练像素级预测?
## 初始化流?- `BUILDER.build(mllm/tokenizer/grounding_encoder)`：利?xtuner 的注册器根据配置字典构建对象?- LoRA 支持：如?`self.mllm.use_llm_lora` 为真，会解冻 `lm_head` ?`embed_tokens`，保?LoRA 在语言输出头生效?- Grounding encoder：整体冻结，单独解冻 `mask_decoder` 与可能存在的跨模态注意力 (`cma`) 层，以便只微调掩码生成相关参数?- `self.text_hidden_fcs`：两层线性层 + ReLU + Dropout，将 LLM 的隐藏维度映射到 mask decoder ?transformer 维度，起到把文本 token embedding 转换成提?embedding 的作用?- `_add_special_tokens`：向 tokenizer 添加 `[SEG]` 特殊 token，并记录其下?`self.seg_token_idx`，用于标记输出序列中触发掩码生成的位置?
## 核心方法拆解

### `_generate_and_postprocess_masks`
- 输入：处理后的文本提?embedding 列表、图?encoder 输出、resize 信息?- 步骤?  1. 调用 `prompt_encoder` 生成稀?稠密提示；此?`text_embeds` 就是?`[SEG]` token 对应?LLM 隐藏态经?`self.text_hidden_fcs` 映射后的向量?  2. `mask_decoder` 结合图像 embedding 与提?embeddings 生成低分辨率掩码?  3. `postprocess_masks` 按原图尺寸恢复掩码分辨率?- 本质：借助 Grounding-SAM 的文本提示接口，?LLM 输出的语义指令转为分割掩码?
### `state_dict`
- 自定义保存逻辑：根据是否使?LoRA 或冻结策略，筛?转换需要保存的权重?- `get_peft_model_state_dict` 用于提取 LoRA adapter 的权重减?checkpoint 体积?- 额外保存 `mask_decoder`、可能存在的 `cma`、`text_hidden_fcs` 以及 `lm_head` 等权重，确保推理时分割能力完整?
### `compute_loss`
- 数据准备?  - `g_pixel_values`：提供给 grounding encoder 的原始图像张量序列?  - `gt_masks`：对应的语义/实例掩码?  - `input_ids`：多模态模型的输入 token?- 执行?  1. 调用 `self.mllm(..., mode="loss")` 得到 LLM 输出及隐藏态?  2. 若无标注掩码（比如仅计算语言损失），构造假数据避免报错?  3. 使用 `self.seg_token_idx` 在输入序列中定位 `[SEG]` 的位置，提取最后一层隐藏态并送入 `self.text_hidden_fcs`，得到掩码提?embedding?  4. 图像前处理：调用 `grounding_encoder.preprocess`、`image_encoder` 得到图像表示?  5. 调用 `_generate_and_postprocess_masks` 生成掩码?  6. 计算 mask ?dice 损失，另外直接使用语言模型的交叉熵损失?- 原理：让 LLM 在生成文本的同时输出 `[SEG]` token，对应的隐藏态驱?SAM 掩码解码器生成像素级分割，实现文本引导的视觉分割联合训练?
### `predict`
- 构?`generation_config`，调?`self.mllm.generate` 获取生成文本与隐藏态?- 截取输出序列?`[SEG]` token 的隐藏态；若没?`[SEG]`，直接返回文本结果?- 与训练流程相同：使用 `text_hidden_fcs` 映射隐藏态，?Grounding-SAM 的掩码生成流程?- 返回?  - `pred_mask_logits`：模型预测的掩码 logits?  - `output_text`：生成的回答文本?
### 梯度检查点控制
- 只对语言模型部分开?关闭梯度检查点，以控制显存?
## 整体数据流（训练阶段?1. 输入包含图像 (`pixel_values`/`g_pixel_values`)、文?token、掩码标注?2. 多模态模型产生语言输出与隐藏态，并根?`[SEG]` token 的位置抽?`seg_embeds`?3. `seg_embeds` 作为文本提示驱动 Grounding-SAM mask decoder 生成掩码?4. 计算掩码损失 + 文本生成损失，实现语言与视觉分割的联合学习?
## 关键机制原理
- **特殊 token 驱动分割**：通过在输出中插入 `[SEG]`，让 LLM 在生成文本时同步输出分割提示 embedding，桥接语言指令和像素掩码?- **冻结策略**：大部分 Grounding-SAM 模块冻结，只训练 mask decoder（及可?CMA）、`text_hidden_fcs`，以?LLM ?LoRA/必要头部，以减少参数量、稳住原模型能力?- **LoRA 支持**：针对视?encoder、语言模型分别提供 LoRA 权重提取逻辑，方便快速适配不同训练策略?- **多任务损?*：结合语言损失（`output.loss`）与掩码损失（BCE + Dice），确保模型同时学会回答问题与给出分割结果?
## 与项目其他模块的关系
- `projects/salma/configs/*.py` 会配?`mllm`、`grounding_encoder`、损失函数等参数，驱?`LisaModel` 的构造?- 训练/推理脚本（如 `tools/train.py`、`demo/predict-img.py`）最终加载该模型，实?Sa2VA-MaskCMA 的文?视觉分割能力?
> 提示：阅读相关配置和 Grounding-SAM 模块（例?`third_parts/sam2/*`）可以进一步理解图像特征编码与掩码生成的细节?