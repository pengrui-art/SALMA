\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\citation{sa2va}
\citation{lira}
\citation{qwenvlplus}
\citation{internvl25}
\citation{sam2}
\citation{glamm}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{sa2va}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{lira}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{qwenvlplus}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{internvl25}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{sam2}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{glamm}{{1}{1}{section.1}}}
\citation{lisa}
\citation{glamm}
\citation{sa2va}
\citation{sam2}
\citation{lira}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Addressing Attention Hallucination in Complex Spatial Grounding.} Current MLLMs like OMG-LLaVA (Center column) often fail to ground objects defined by fine-grained spatial constraints or interactions, leading to \textit  {attention drift} towards more salient but incorrect targets. Our proposed \textbf  {Structure-Aware Sa2VA} (Right column) explicitly injects structural priors, enabling precise localization of objects defined by (Top row) relative depth, (Middle row) spatial ordering, and (Bottom row) human-object interactions. Key spatial terms in instructions are highlighted in \textbf  {bold}. Red regions indicate predicted segmentation masks.}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{2}{\textbf {Addressing Attention Hallucination in Complex Spatial Grounding.} Current MLLMs like OMG-LLaVA (Center column) often fail to ground objects defined by fine-grained spatial constraints or interactions, leading to \textit {attention drift} towards more salient but incorrect targets. Our proposed \textbf {Structure-Aware Sa2VA} (Right column) explicitly injects structural priors, enabling precise localization of objects defined by (Top row) relative depth, (Middle row) spatial ordering, and (Bottom row) human-object interactions. Key spatial terms in instructions are highlighted in \textbf {bold}. Red regions indicate predicted segmentation masks}{figure.caption.1}{}}
\newlabel{fig:teaser@cref}{{[figure][1][]1}{[1][1][]2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Unified Multimodal Segmentation}{2}{subsection.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{lisa}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{glamm}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{sa2va}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{sam2}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{lira}{{2}{2.1}{subsection.2.1}}}
\citation{sam2}
\citation{omgllava}
\citation{psalm}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Universal and Interactive Segmentation}{3}{subsection.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{sam2}{{3}{2.2}{subsection.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Structure-Aware Vision-Language Alignment}{3}{subsection.2.3}\protected@file@percent }
\@writefile{brf}{\backcite{omgllava}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{psalm}{{3}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminaries}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Mask-Biased Attention (MBA)}{3}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Framework Overview.} \textbf  {Left:} Multi-scale visual features $\mathcal  {F}_v$ and text embeddings are extracted. \textbf  {Middle: Mask-Biased Attention (MBA).} An auxiliary head generates a low-res prior $M_{prior}$, which is transformed via a Sigmoid into a spatial Gate $G$. This gate is multiplied ($\otimes $) with the Cross-Attention map to suppress noise. \textbf  {Right: Alignment Constraints.} We enforce semantic alignment via Text-Mask Contrastive (TMC) Loss ($\leftrightarrow $) and structural precision via Boundary Loss (highlighted contours).}}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:framework}{{2}{4}{\textbf {Framework Overview.} \textbf {Left:} Multi-scale visual features $\mathcal {F}_v$ and text embeddings are extracted. \textbf {Middle: Mask-Biased Attention (MBA).} An auxiliary head generates a low-res prior $M_{prior}$, which is transformed via a Sigmoid into a spatial Gate $G$. This gate is multiplied ($\otimes $) with the Cross-Attention map to suppress noise. \textbf {Right: Alignment Constraints.} We enforce semantic alignment via Text-Mask Contrastive (TMC) Loss ($\leftrightarrow $) and structural precision via Boundary Loss (highlighted contours)}{figure.caption.2}{}}
\newlabel{fig:framework@cref}{{[figure][2][]2}{[1][3][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fine-grained Semantic-Structure Alignment}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:alignment}{{3.3}{4}{Fine-grained Semantic-Structure Alignment}{subsection.3.3}{}}
\newlabel{sec:alignment@cref}{{[subsection][3][3]3.3}{[1][4][]4}{}{}{}}
\citation{pixellm}
\citation{lisa}
\citation{lisaglee}
\citation{glamm}
\citation{gsva}
\citation{omgllava}
\citation{psalm}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Mask-Biased Attention (MBA) Mechanism.} The module leverages a learned spatial prior $G$ (Middle) to gate the noisy raw attention $A_{raw}$ (Left). By applying element-wise multiplication $\otimes $, the resulting attention $\hat  {A}$ (Right) is strictly confined to the target object.}}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:mba_module}{{3}{5}{\textbf {Mask-Biased Attention (MBA) Mechanism.} The module leverages a learned spatial prior $G$ (Middle) to gate the noisy raw attention $A_{raw}$ (Left). By applying element-wise multiplication $\otimes $, the resulting attention $\hat {A}$ (Right) is strictly confined to the target object}{figure.caption.3}{}}
\newlabel{fig:mba_module@cref}{{[figure][3][]3}{[1][3][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Unified Training Objective}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation Details}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Datasets Setup}{5}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Comparison with State-of-the-Art Methods.} We report cIoU for image referring segmentation tasks, $\mathcal  {J}\&\mathcal  {F}$ for video segmentation tasks, and standard metrics for multimodal understanding. The best results are highlighted in \textbf  {bold}. Structure-Aware Sa2VA achieves State-of-the-Art performance on video segmentation tasks and complex referring scenarios (e.g., RefCOCOg), while maintaining competitive general understanding capabilities.}}{6}{table.caption.4}\protected@file@percent }
\newlabel{tab:sota_comparison}{{1}{6}{\textbf {Comparison with State-of-the-Art Methods.} We report cIoU for image referring segmentation tasks, $\mathcal {J}\&\mathcal {F}$ for video segmentation tasks, and standard metrics for multimodal understanding. The best results are highlighted in \textbf {bold}. Structure-Aware Sa2VA achieves State-of-the-Art performance on video segmentation tasks and complex referring scenarios (e.g., RefCOCOg), while maintaining competitive general understanding capabilities}{table.caption.4}{}}
\newlabel{tab:sota_comparison@cref}{{[table][1][]1}{[1][5][]6}{}{}{}}
\@writefile{brf}{\backcite{pixellm}{{6}{1}{table.caption.4}}}
\@writefile{brf}{\backcite{lisa}{{6}{1}{table.caption.4}}}
\@writefile{brf}{\backcite{lisaglee}{{6}{1}{table.caption.4}}}
\@writefile{brf}{\backcite{glamm}{{6}{1}{table.caption.4}}}
\@writefile{brf}{\backcite{gsva}{{6}{1}{table.caption.4}}}
\@writefile{brf}{\backcite{omgllava}{{6}{1}{table.caption.4}}}
\@writefile{brf}{\backcite{psalm}{{6}{1}{table.caption.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison with State-of-the-Art}{6}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Ablation Study}{6}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Qualitative Comparison on Complex Referring Expressions.} \textbf  {Top Row:} Our model successfully handles complex spatial relationships (e.g., ``small'', ``to its left''), whereas the baseline incorrectly segments the distracting neighbor. \textbf  {Bottom Row:} In highly cluttered scenes (pile of fries), our method produces masks with superior boundary crispness and completeness compared to the fragmented outputs of the baseline.}}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:viz_comparison}{{4}{7}{\textbf {Qualitative Comparison on Complex Referring Expressions.} \textbf {Top Row:} Our model successfully handles complex spatial relationships (e.g., ``small'', ``to its left''), whereas the baseline incorrectly segments the distracting neighbor. \textbf {Bottom Row:} In highly cluttered scenes (pile of fries), our method produces masks with superior boundary crispness and completeness compared to the fragmented outputs of the baseline}{figure.caption.6}{}}
\newlabel{fig:viz_comparison@cref}{{[figure][4][]4}{[1][7][]7}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Ablation Study of Components.} We progressively add Mask-Biased Attention (MBA), Text-Mask Contrastive (TMC) and Boundary Loss.}}{7}{table.caption.5}\protected@file@percent }
\newlabel{tab:ablation_component}{{2}{7}{\textbf {Ablation Study of Components.} We progressively add Mask-Biased Attention (MBA), Text-Mask Contrastive (TMC) and Boundary Loss}{table.caption.5}{}}
\newlabel{tab:ablation_component@cref}{{[table][2][]2}{[1][7][]7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Visualization of Cross-Modal Attention.} We visualize the attention for the instruction \textit  {``Segment the fry...''}. (b) Baseline exhibits diffused attention. (c) MBA concentrates focus on the target.}}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:attn_map}{{5}{7}{\textbf {Visualization of Cross-Modal Attention.} We visualize the attention for the instruction \textit {``Segment the fry...''}. (b) Baseline exhibits diffused attention. (c) MBA concentrates focus on the target}{figure.caption.7}{}}
\newlabel{fig:attn_map@cref}{{[figure][5][]5}{[1][7][]7}{}{}{}}
\bibstyle{plain}
\bibdata{egbib}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Analysis of Feature Modulation.} Comparing our simplified spatial gating (MBA) vs. complex FiLM modulation. \textcolor {red}{Red text} indicates a performance drop.}}{8}{table.caption.8}\protected@file@percent }
\newlabel{tab:ablation_film}{{3}{8}{\textbf {Analysis of Feature Modulation.} Comparing our simplified spatial gating (MBA) vs. complex FiLM modulation. \textcolor {red}{Red text} indicates a performance drop}{table.caption.8}{}}
\newlabel{tab:ablation_film@cref}{{[table][3][]3}{[1][7][]8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Visualization}{8}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Limitations}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}\protected@file@percent }
\gdef \@abspage@last{8}
