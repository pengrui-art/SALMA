\begin{thebibliography}{10}

\bibitem{qwenvlplus}
Alibaba Group.
\newblock Qwen-vl-plus: Enhancing vision-language models with unified
  understanding, 2024.

\bibitem{lisa}
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya
  Jia.
\newblock Lisa: Reasoning segmentation via large language model.
\newblock {\em arXiv preprint arXiv:2308.00692}, 2023.

\bibitem{lira}
Xiangtai Li et~al.
\newblock Lira: Inferring segmentation in large multi-modal models with local
  interleaved region assistance.
\newblock {\em arXiv preprint arXiv:2501.00000}, 2025.

\bibitem{internvl25}
OpenGVLab.
\newblock Internvl 2.5: Advanced multimodal large language models, 2025.

\bibitem{glamm}
Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan,
  Shahbahd Fahad, and Fahad~Shahbaz Khan.
\newblock Glamm: Pixel grounding large multimodal model.
\newblock {\em CVPR}, 2024.

\bibitem{sam2}
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali,
  Tengyu Ma, Haitham Khedr, Roman R{\"a}dle, Chloe Rolland, Laura Gustafson,
  et~al.
\newblock Sam 2: Segment anything in images and videos.
\newblock {\em arXiv preprint arXiv:2408.00714}, 2024.

\bibitem{pixellm}
Zhongwei Ren, Zhilin Ji, Graham Lan, Zhaofan Wang, Yin Cui, Wei Zhai, and
  Jiashi Feng.
\newblock Pixellm: Pixel reasoning with large multimodal models.
\newblock {\em CVPR}, 2024.

\bibitem{lisaglee}
Junfeng Wu, Yi~Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, and Song Bai.
\newblock Glee: General object foundation model for images and videos at scale.
\newblock {\em CVPR}, 2024.

\bibitem{gsva}
Zhuofan Xia, Xuran Han, Ysheng Xue, and Wenqiang Zhang.
\newblock Gsva: Generalized segmentation via multimodal large language models.
\newblock {\em CVPR}, 2024.

\bibitem{sa2va}
Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji,
  Yunhai Tong, Lu~Qi, Jiashi Feng, and Ming-Hsuan Yang.
\newblock Sa2va: Marrying sam2 with llava for dense grounded understanding of
  images and videos, 2025.

\bibitem{omgllava}
Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Wan, and Ming-Hsuan Yang.
\newblock Omg-llava: Bridging image-level, object-level, pixel-level reasoning
  and understanding.
\newblock {\em arXiv preprint arXiv:2406.19389}, 2024.

\bibitem{psalm}
Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Wan, and Ming-Hsuan Yang.
\newblock Psalm: Pixelwise segmentation with large multi-modal model.
\newblock {\em arXiv preprint arXiv:2403.14598}, 2024.

\end{thebibliography}
