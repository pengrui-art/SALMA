\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sa2va}
\citation{lira}
\citation{qwen}
\citation{internvl}
\citation{sam2}
\citation{omgllava}
\citation{omgllava}
\citation{mevis}
\citation{seggpt}
\@writefile{toc}{\contentsline {title}{SALMA: Mask-Guided Structure-Aware Alignment for Referring Segmentation}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Anonymous ECCV 2026{} Submission}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{davis2017}
\citation{refcocog}
\citation{gpt4}
\citation{llava15}
\citation{internvl,qwen}
\citation{llavanext}
\citation{videollava,videolisa}
\citation{mmbench,mme,seedbench}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Addressing Attention Hallucination.} Unlike OMG-LLaVA~\cite  {omgllava} (Center), SALMA (Right) uses structural priors to precisely ground objects with complex spatial constraints (e.g., depth, ordering), avoiding attention drift.}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{2}{\textbf {Addressing Attention Hallucination.} Unlike OMG-LLaVA~\cite {omgllava} (Center), SALMA (Right) uses structural priors to precisely ground objects with complex spatial constraints (e.g., depth, ordering), avoiding attention drift}{figure.caption.2}{}}
\newlabel{fig:teaser@cref}{{[figure][1][]1}{[1][1][]2}{}{}{}}
\citation{lavt}
\citation{cris}
\citation{referformer}
\citation{seem}
\citation{seggpt}
\citation{sam2}
\citation{lisa,pixellm,glamm,gsva,sa2va,lira}
\citation{lisa}
\citation{pixellm}
\citation{glamm}
\citation{psalm}
\citation{maskgrounding}
\citation{lira}
\citation{sa2va}
\citation{sam2}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Formulation}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:preliminaries}{{3.1}{3}{Problem Formulation}{subsection.3.1}{}}
\newlabel{sec:preliminaries@cref}{{[subsection][1][3]3.1}{[1][3][]3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Framework Overview.} \textbf  {(a) Shared Visual Encoder.} Multi-scale visual features $\mathcal  {F}_v$ and text embeddings are extracted. \textbf  {(b) Mask-Biased Attention.} Instead of a separate auxiliary head, we perform a pre-pass inference using the shared SAM-2 decoder to generate a prior $\mathcal  {M}_{prior}$. This prior is transformed via a Sigmoid into a spatial Gate $G$, which is multiplied ($\otimes $) with the Cross-Attention output features to suppress noise. \textbf  {(c) Fine-grained Alignment.} We enforce semantic alignment via TMC Loss ($\leftrightarrow $) and structural precision via Boundary Loss (highlighted contours).}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:framework}{{2}{4}{\textbf {Framework Overview.} \textbf {(a) Shared Visual Encoder.} Multi-scale visual features $\mathcal {F}_v$ and text embeddings are extracted. \textbf {(b) Mask-Biased Attention.} Instead of a separate auxiliary head, we perform a pre-pass inference using the shared SAM-2 decoder to generate a prior $\mathcal {M}_{prior}$. This prior is transformed via a Sigmoid into a spatial Gate $G$, which is multiplied ($\otimes $) with the Cross-Attention output features to suppress noise. \textbf {(c) Fine-grained Alignment.} We enforce semantic alignment via TMC Loss ($\leftrightarrow $) and structural precision via Boundary Loss (highlighted contours)}{figure.caption.3}{}}
\newlabel{fig:framework@cref}{{[figure][2][]2}{[1][3][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Mask-Biased Attention}{5}{subsection.3.2}\protected@file@percent }
\newlabel{sec:mba}{{3.2}{5}{Mask-Biased Attention}{subsection.3.2}{}}
\newlabel{sec:mba@cref}{{[subsection][2][3]3.2}{[1][5][]5}{}{}{}}
\citation{lisa}
\citation{glamm}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {The Mask-Biased Attention (MBA) Mechanism.} We visualize the transformation from the noisy global attention $A_{\text  {raw}}$ to the refined structure-aware output $\hat  {A}$. The \textbf  {Mask Prior ($G$)}, acting as a soft spatial filter, is element-wise multiplied ($\otimes $) with the raw features. This process effectively \textbf  {suppresses background hallucinations} and confines semantic reasoning strictly within the target's topology.}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:mba_module}{{3}{6}{\textbf {The Mask-Biased Attention (MBA) Mechanism.} We visualize the transformation from the noisy global attention $A_{\text {raw}}$ to the refined structure-aware output $\hat {A}$. The \textbf {Mask Prior ($G$)}, acting as a soft spatial filter, is element-wise multiplied ($\otimes $) with the raw features. This process effectively \textbf {suppresses background hallucinations} and confines semantic reasoning strictly within the target's topology}{figure.caption.4}{}}
\newlabel{fig:mba_module@cref}{{[figure][3][]3}{[1][5][]6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fine-grained Alignment}{6}{subsection.3.3}\protected@file@percent }
\newlabel{sec:alignment}{{3.3}{6}{Fine-grained Alignment}{subsection.3.3}{}}
\newlabel{sec:alignment@cref}{{[subsection][3][3]3.3}{[1][6][]6}{}{}{}}
\citation{refcoco}
\citation{refcoco}
\citation{refcocog}
\citation{davis2017}
\citation{ytvos}
\citation{mevis}
\citation{mme}
\citation{mmbench}
\citation{seedbench}
\citation{llava15}
\citation{videollava}
\citation{llamavid}
\citation{mplugowl3}
\citation{internvl}
\citation{pixellm}
\citation{pixellm}
\citation{lasagna}
\citation{lisa}
\citation{lisaglee}
\citation{glamm}
\citation{llavag}
\citation{gsva}
\citation{omgllava}
\citation{psalm}
\citation{videolisa}
\citation{visa}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Unified Training Objective}{7}{subsection.3.4}\protected@file@percent }
\citation{internvl25new}
\citation{refcoco,refcocog}
\citation{visa}
\citation{ytvos,davis2017}
\citation{llava15}
\citation{chatunivi24}
\citation{osprey24}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Overall Training Pipeline}}{8}{algorithm.1}\protected@file@percent }
\newlabel{alg:main}{{1}{8}{Overall Training Pipeline}{algorithm.1}{}}
\newlabel{alg:main@cref}{{[algorithm][1][]1}{[1][7][]8}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation Details}{8}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Comparison with State-of-the-Art Methods.} We report cIoU for image referring segmentation tasks, $\mathcal  {J}\&\mathcal  {F}$ for video segmentation tasks, and standard metrics for multimodal understanding. The best results are highlighted in \textbf  {bold}. SALMA achieves leading performance among unified MLLMs on video segmentation tasks and complex referring scenarios (e.g., RefCOCOg), while maintaining competitive general understanding capabilities.}}{9}{table.caption.5}\protected@file@percent }
\newlabel{tab:sota_comparison}{{1}{9}{\textbf {Comparison with State-of-the-Art Methods.} We report cIoU for image referring segmentation tasks, $\mathcal {J}\&\mathcal {F}$ for video segmentation tasks, and standard metrics for multimodal understanding. The best results are highlighted in \textbf {bold}. SALMA achieves leading performance among unified MLLMs on video segmentation tasks and complex referring scenarios (e.g., RefCOCOg), while maintaining competitive general understanding capabilities}{table.caption.5}{}}
\newlabel{tab:sota_comparison@cref}{{[table][1][]1}{[1][7][]9}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Evaluation Benchmarks}{9}{subsection.4.2}\protected@file@percent }
\citation{psalm}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Ablation Study of Components.} We progressively add Mask-Biased Attention, Text-Mask Contrastive and Boundary Loss.}}{10}{table.caption.6}\protected@file@percent }
\newlabel{tab:ablation_component}{{2}{10}{\textbf {Ablation Study of Components.} We progressively add Mask-Biased Attention, Text-Mask Contrastive and Boundary Loss}{table.caption.6}{}}
\newlabel{tab:ablation_component@cref}{{[table][2][]2}{[1][10][]10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison with Leading MLLMs}{10}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Ablation Study}{10}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Analysis of Feature Modulation.} Comparing our simplified spatial gating vs. complex FiLM modulation. \textcolor {red}{Red text} indicates a performance drop relative to our full model.}}{11}{table.caption.7}\protected@file@percent }
\newlabel{tab:ablation_film}{{3}{11}{\textbf {Analysis of Feature Modulation.} Comparing our simplified spatial gating vs. complex FiLM modulation. \textcolor {red}{Red text} indicates a performance drop relative to our full model}{table.caption.7}{}}
\newlabel{tab:ablation_film@cref}{{[table][3][]3}{[1][10][]11}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Saliency Bias Analysis on RefCOCOg (Val).} We report cIoU scores partitioned by object area ratio ($<1\%, <2\%, <5\%$), where $N$ denotes the number of samples in each subset. Our method demonstrates robust improvements on small objects ($<2\%$ and $<5\%$), effectively countering saliency bias concerns.}}{11}{table.caption.8}\protected@file@percent }
\newlabel{tab:ablation_saliency}{{4}{11}{\textbf {Saliency Bias Analysis on RefCOCOg (Val).} We report cIoU scores partitioned by object area ratio ($<1\%, <2\%, <5\%$), where $N$ denotes the number of samples in each subset. Our method demonstrates robust improvements on small objects ($<2\%$ and $<5\%$), effectively countering saliency bias concerns}{table.caption.8}{}}
\newlabel{tab:ablation_saliency@cref}{{[table][4][]4}{[1][11][]11}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Computational Cost Analysis on 1024$\times $1024 resolution.} We report parameters and FLOPs for major components. The SALMA pre-pass (Gating + Decoder) introduces negligible overhead ($\sim $0.04\%) relative to the backbone encoders.}}{12}{table.caption.9}\protected@file@percent }
\newlabel{tab:flops}{{5}{12}{\textbf {Computational Cost Analysis on 1024$\times $1024 resolution.} We report parameters and FLOPs for major components. The SALMA pre-pass (Gating + Decoder) introduces negligible overhead ($\sim $0.04\%) relative to the backbone encoders}{table.caption.9}{}}
\newlabel{tab:flops@cref}{{[table][5][]5}{[1][11][]12}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Visualization}{12}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{12}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{12}{Discussion}{section.5}{}}
\newlabel{sec:discussion@cref}{{[section][5][]5}{[1][12][]12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Qualitative Comparison on Complex Referring Expressions.} \textbf  {Top Row:} Our model successfully handles complex spatial relationships (e.g., ``small'', ``to its left''), whereas the baseline incorrectly segments the distracting neighbor. \textbf  {Bottom Row:} In highly cluttered scenes (pile of fries), our method produces masks with superior boundary crispness and completeness compared to the fragmented outputs of the baseline.}}{13}{figure.caption.10}\protected@file@percent }
\newlabel{fig:viz_comparison}{{4}{13}{\textbf {Qualitative Comparison on Complex Referring Expressions.} \textbf {Top Row:} Our model successfully handles complex spatial relationships (e.g., ``small'', ``to its left''), whereas the baseline incorrectly segments the distracting neighbor. \textbf {Bottom Row:} In highly cluttered scenes (pile of fries), our method produces masks with superior boundary crispness and completeness compared to the fragmented outputs of the baseline}{figure.caption.10}{}}
\newlabel{fig:viz_comparison@cref}{{[figure][4][]4}{[1][12][]13}{}{}{}}
\bibstyle{splncs04}
\bibdata{egbib}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Visualization of the structure-aware ``Spotlight Effect.''} We compare the MLLM cross-attention maps of the Baseline (b) versus SALMA (c). As highlighted by the red circles, the Baseline model fails to isolate the specific target, allowing attention to drift to nearby background clutter (e.g., leaking away from the carrots in Row 1) or semantically similar surroundings (e.g., scattering across adjacent french fries in Row 2). SALMA successfully rectifies this via Mask-Biased Attention, utilizing the structural prior to tightly focus the model's attention solely on the queried topology, eliminating background noise.}}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:attn_map}{{5}{14}{\textbf {Visualization of the structure-aware ``Spotlight Effect.''} We compare the MLLM cross-attention maps of the Baseline (b) versus SALMA (c). As highlighted by the red circles, the Baseline model fails to isolate the specific target, allowing attention to drift to nearby background clutter (e.g., leaking away from the carrots in Row 1) or semantically similar surroundings (e.g., scattering across adjacent french fries in Row 2). SALMA successfully rectifies this via Mask-Biased Attention, utilizing the structural prior to tightly focus the model's attention solely on the queried topology, eliminating background noise}{figure.caption.11}{}}
\newlabel{fig:attn_map@cref}{{[figure][5][]5}{[1][12][]14}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{14}{section.6}\protected@file@percent }
\bibcite{qwen}{1}
\bibcite{internvl25new}{2}
\bibcite{internvl}{3}
\bibcite{maskgrounding}{4}
\bibcite{mevis}{5}
\bibcite{mme}{6}
\bibcite{chatunivi24}{7}
\bibcite{videolisa}{8}
\bibcite{refcoco}{9}
\bibcite{lisa}{10}
\bibcite{seedbench}{11}
\bibcite{lira}{12}
\bibcite{llamavid}{13}
\bibcite{videollava}{14}
\bibcite{llava15}{15}
\bibcite{llavanext}{16}
\bibcite{mmbench}{17}
\bibcite{refcocog}{18}
\bibcite{gpt4}{19}
\bibcite{davis2017}{20}
\bibcite{glamm}{21}
\bibcite{sam2}{22}
\bibcite{pixellm}{23}
\bibcite{seggpt}{24}
\bibcite{cris}{25}
\bibcite{lasagna}{26}
\bibcite{referformer}{27}
\bibcite{lisaglee}{28}
\bibcite{gsva}{29}
\bibcite{ytvos}{30}
\bibcite{visa}{31}
\bibcite{lavt}{32}
\bibcite{osprey24}{33}
\bibcite{mplugowl3}{34}
\bibcite{sa2va}{35}
\bibcite{llavag}{36}
\bibcite{omgllava}{37}
\bibcite{psalm}{38}
\bibcite{seem}{39}
\gdef \@abspage@last{16}
